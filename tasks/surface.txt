import sys
sys.path.append('D:\\Master\\Implementing Transformer\\transformer_project\\modelling')
from tokenizer import BPETokenizer

Questions:
1. On which corpus sould the BPETokenizer be trained?
2. Should train, test, val be trained on the same corpus?
3. How to use vocab and merges in from_pretrained method?
4. How to implement MultiHeadAttention?
5. In from_pretrained specify special tokens again?
6. What should we done in case of the Word embeddings? (Practical 6)
7. Get a conformation of the proofs.
8. Is also one dropout layer possible
9. Pointwise FF as a seperate nn.Module?