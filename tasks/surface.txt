Questions:
1. Explain special_token_map again where to add special tokens?
2. Sequence length?
3. target_input and target_output correct?
4. Correctness test_8 had to add tolerance?
5. Should we have two embd and pos_enc or is one with share weights enough?
6. What are the benefits for sharing weights in word embeddings?
7. How to create the input mask?
8. get_last_lr return List or index 0?