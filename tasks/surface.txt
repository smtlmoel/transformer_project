Questions:
1. Explain special_token_map again where to add special tokens?
2. Sequence length?
3. target_input and target_output correct?
4. Correctness test_8 had to add tolerance?
5. Should we habe two embd and pos_enc and share weights or is one enough?
6. What are the benefits for sharing weights in word embeddings?
7. Can we use the LRSCheduler Class from pytorch? If yes can we access the _step_count?
8. How to create the input mask?
9. get_last_lr List or index 0?